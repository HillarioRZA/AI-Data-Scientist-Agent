# Data Whisperer v1.0
## AI-Powered Data Analysis Platform

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [API Documentation](#api-documentation)
- [Services](#services)
- [Project Structure](#project-structure)
- [Usage Examples](#usage-examples)
- [Development](#development)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

**Data Whisperer** adalah platform AI-powered yang mengintegrasikan Exploratory Data Analysis (EDA), Machine Learning (ML), dan Retrieval Augmented Generation (RAG) dalam satu sistem terpadu. Platform ini dirancang untuk memudahkan analisis data dengan bantuan AI Agent yang dapat memahami konteks data dan memberikan insights yang actionable.

### Key Highlights
- ğŸ¤– **AI Agent** dengan kemampuan reasoning dan acting
- ğŸ“Š **Comprehensive EDA** dengan 12+ analisis statistik
- ğŸ§  **Machine Learning Pipeline** otomatis dengan hyperparameter tuning
- ğŸ“„ **RAG System** untuk PDF document analysis
- ğŸ§  **Memory Management** dengan session-based context
- ğŸ”„ **Real-time Processing** dengan FastAPI backend

---

## âœ¨ Features

### ğŸ¤– AI Agent Services
- **ReAct Architecture**: Reasoning + Acting untuk intelligent decision making
- **Intent Classification**: Automatic tool selection berdasarkan user intent
- **Multimodal Interpretation**: JSON dan image interpretation
- **Data-Aware Agent**: Mencegah halusinasi dengan data context awareness
- **Conversational Interface**: Natural language interaction

### ğŸ“Š Exploratory Data Analysis (EDA)
- **Data Description**: Statistical summaries dan data profiling
- **Visualization**: Histograms, heatmaps, correlation matrices
- **Outlier Detection**: Statistical outlier identification
- **Skewness Analysis**: Distribution analysis
- **Categorical Insights**: Categorical variable analysis
- **Target Analysis**: Target variable analysis
- **VIF Analysis**: Variance Inflation Factor calculation
- **Custom Visualization**: User-defined plot generation

### ğŸ§  Machine Learning Pipeline
- **Automatic Problem Detection**: Classification vs Regression detection
- **Data Preprocessing**: Imputation, encoding, scaling
- **Model Training**: RandomForest dengan hyperparameter tuning
- **Model Evaluation**: Comprehensive metrics calculation
- **Feature Importance**: Explainable AI (XAI) features
- **Model Persistence**: Session-based model storage
- **Prediction**: New data prediction dengan trained models

### ğŸ“„ RAG (Retrieval Augmented Generation)
- **PDF Processing**: Text extraction dari PDF documents
- **Vector Store**: FAISS-based document indexing
- **Question Answering**: Context-aware Q&A system
- **Document Analysis**: Multi-document analysis capabilities

### ğŸ§  Memory Management
- **Short-Term Memory**: Session-based conversation context
- **Long-Term Memory**: Persistent data storage dengan TinyDB
- **Session Management**: Isolated user sessions
- **Data Persistence**: Model dan dataset storage

---

## ğŸ—ï¸ Architecture

### System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   FastAPI       â”‚    â”‚   AI Services   â”‚
â”‚   (React)       â”‚â—„â”€â”€â–ºâ”‚   Backend       â”‚â—„â”€â”€â–ºâ”‚   (LangChain)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Core Services â”‚
                       â”‚   - EDA         â”‚
                       â”‚   - ML          â”‚
                       â”‚   - RAG         â”‚
                       â”‚   - Memory      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Architecture
- **API Layer**: FastAPI dengan modular routers
- **Service Layer**: Core business logic
- **Memory Layer**: Session dan persistent storage
- **Integration Layer**: External services (LLM, embeddings)

---

## ğŸš€ Installation

### Prerequisites
- Python 3.10+
- pip (Python package manager)

### Step 1: Clone Repository
```bash
git clone https://github.com/your-username/data-whisperer.git
cd data-whisperer
```

### Step 2: Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Environment Setup
Create `.env` file:
```env
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
```

### Step 5: Run Application
```bash
# Development server
uvicorn backend.api.main:app --reload --host 0.0.0.0 --port 8000

# Production server
uvicorn backend.api.main:app --host 0.0.0.0 --port 8000
```

---

## ğŸš€ Quick Start

### 1. Start the Server
```bash
uvicorn backend.api.main:app --reload
```

### 2. Access the API
- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/

### 3. Upload Data
```python
import requests

# Upload CSV file
with open('data.csv', 'rb') as f:
    response = requests.post(
        'http://localhost:8000/eda/upload',
        files={'file': f}
    )
```

### 4. Run EDA Analysis
```python
# Get data description
response = requests.get('http://localhost:8000/eda/describe')
print(response.json())
```

### 5. Interact with AI Agent
```python
# Ask AI agent
response = requests.post(
    'http://localhost:8000/agent/execute',
    json={
        'session_id': 'your-session-id',
        'prompt': 'Analyze this data and create a visualization'
    }
)
```

---

## ğŸ“š API Documentation

### Base URL
```
http://localhost:8000
```

### Main Endpoints

#### ğŸ  Health Check
```http
GET /
```

#### ğŸ“Š EDA Endpoints
```http
POST /eda/upload                    # Upload CSV file
GET  /eda/describe                  # Data description
GET  /eda/correlation-heatmap       # Correlation heatmap
GET  /eda/histogram/{column}         # Histogram for column
GET  /eda/outliers                  # Outlier detection
GET  /eda/skewness                  # Skewness analysis
GET  /eda/categorical               # Categorical analysis
GET  /eda/target-analysis           # Target variable analysis
GET  /eda/full-profile              # Complete data profile
```

#### ğŸ¤– Agent Endpoints
```http
POST /agent/execute                 # Execute agent action
POST /agent/visualization           # Create custom visualization
```

#### ğŸ§  ML Endpoints
```http
POST /ml/run-pipeline               # Run ML pipeline
POST /ml/predict                    # Make prediction
POST /ml/tuned-pipeline             # Run tuned pipeline
GET  /ml/feature-importance         # Get feature importance
GET  /ml/download-model             # Download model artifacts
```

---

## ğŸ”§ Services

### ğŸ“Š EDA Service
**Location**: `backend/services/eda/`

**Core Functions**:
- `get_csv_description()`: Statistical data description
- `get_outliers()`: Outlier detection
- `get_skewness()`: Distribution analysis
- `get_categorical_insights()`: Categorical analysis
- `analyze_target()`: Target variable analysis
- `run_full_data_profile()`: Comprehensive profiling
- `calculate_vif()`: VIF analysis
- `generate_custom_plot()`: Custom visualization

### ğŸ¤– Agent Service
**Location**: `backend/services/agent/`

**Core Functions**:
- `run_agent_flow()`: Main agent workflow
- `get_agent_plan()`: Intent classification dan tool selection
- `execute_tool()`: Tool execution engine
- `get_interpretation()`: Result interpretation

### ğŸ§  ML Service
**Location**: `backend/services/ml/`

**Core Functions**:
- `detect_problem_type()`: Problem type detection
- `preprocess_data()`: Data preprocessing
- `train_model()`: Model training dengan tuning
- `evaluate_model()`: Model evaluation
- `predict_new_data()`: New data prediction
- `get_feature_importance()`: Feature importance

### ğŸ“„ RAG Service
**Location**: `backend/services/rag/`

**Core Functions**:
- `parse_pdf()`: PDF text extraction
- `create_vector_store()`: Vector store creation
- `get_rag_answer()`: Question answering

### ğŸ§  Memory Service
**Location**: `backend/services/memory/`

**Core Functions**:
- `save_model_data()`: Model persistence
- `save_dataset_path()`: Dataset path storage
- `save_chat_history()`: Chat history storage
- `get_or_create_memory()`: Memory management

---

## ğŸ“ Project Structure

```
Data_Whisperer_v1.0/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â”‚   â”œâ”€â”€ agent_router.py     # Agent endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ eda_router.py       # EDA endpoints
â”‚   â”‚   â”‚   â””â”€â”€ ml_router.py         # ML endpoints
â”‚   â”‚   â”œâ”€â”€ Docs.txt                # API documentation
â”‚   â”‚   â””â”€â”€ Code_docs.txt           # Code documentation
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py             # Agent workflow
â”‚   â”‚   â”‚   â”œâ”€â”€ plan.py             # Intent classification
â”‚   â”‚   â”‚   â”œâ”€â”€ execute_tools.py    # Tool execution
â”‚   â”‚   â”‚   â”œâ”€â”€ interpretation.py  # Result interpretation
â”‚   â”‚   â”‚   â”œâ”€â”€ Docs.txt            # Service documentation
â”‚   â”‚   â”‚   â””â”€â”€ Code_docs.txt       # Code documentation
â”‚   â”‚   â”œâ”€â”€ eda/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py             # EDA functions
â”‚   â”‚   â”‚   â”œâ”€â”€ Docs.txt            # Service documentation
â”‚   â”‚   â”‚   â””â”€â”€ Code_docs.txt       # Code documentation
â”‚   â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”‚   â”œâ”€â”€ evaluator.py        # Model evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ predictor.py        # Prediction
â”‚   â”‚   â”‚   â”œâ”€â”€ preprocessor.py     # Data preprocessing
â”‚   â”‚   â”‚   â”œâ”€â”€ selector.py         # Problem type detection
â”‚   â”‚   â”‚   â”œâ”€â”€ trainer.py          # Model training
â”‚   â”‚   â”‚   â””â”€â”€ Code_docs.txt       # Code documentation
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py           # PDF parsing
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py        # Document retrieval
â”‚   â”‚   â”‚   â”œâ”€â”€ vectorizer.py       # Vector store creation
â”‚   â”‚   â”‚   â””â”€â”€ Code_docs.txt       # Code documentation
â”‚   â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”‚   â”œâ”€â”€ memory_manager.py   # Short-term memory
â”‚   â”‚   â”‚   â”œâ”€â”€ persistent_memory.py # Long-term memory
â”‚   â”‚   â”‚   â””â”€â”€ Code_docs.txt       # Code documentation
â”‚   â”‚   â””â”€â”€ visualization/
â”‚   â”‚       â””â”€â”€ main.py             # Visualization utilities
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ read_csv.py             # CSV utilities
â”‚   â””â”€â”€ core/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset/                    # Sample datasets
â”‚   â”œâ”€â”€ processed/                  # Processed data
â”‚   â”œâ”€â”€ uploaded/                   # User uploads
â”‚   â””â”€â”€ vectorstore/                # Vector stores
â”œâ”€â”€ frontend/                       # React frontend (planned)
â”œâ”€â”€ logs/                           # Application logs
â”œâ”€â”€ saved_models/                   # Trained models
â”œâ”€â”€ user_uploads/                   # User uploaded files
â”œâ”€â”€ memory_db.json                  # Persistent memory database
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ note.txt                        # Project notes
â””â”€â”€ README.md                       # This file
```

---

## ğŸ’¡ Usage Examples

### 1. Data Analysis Workflow

```python
import requests
import json

# Step 1: Upload data
with open('sales_data.csv', 'rb') as f:
    upload_response = requests.post(
        'http://localhost:8000/eda/upload',
        files={'file': f}
    )
session_id = upload_response.json()['session_id']

# Step 2: Get data description
description = requests.get('http://localhost:8000/eda/describe')
print("Data Description:", description.json())

# Step 3: Run EDA analysis
correlation = requests.get('http://localhost:8000/eda/correlation-heatmap')
outliers = requests.get('http://localhost:8000/eda/outliers')
```

### 2. AI Agent Interaction

```python
# Ask AI agent to analyze data
agent_response = requests.post(
    'http://localhost:8000/agent/execute',
    json={
        'session_id': session_id,
        'prompt': 'Create a correlation heatmap and identify outliers'
    }
)
print("Agent Response:", agent_response.json())
```

### 3. Machine Learning Pipeline

```python
# Run ML pipeline
ml_response = requests.post(
    'http://localhost:8000/ml/run-pipeline',
    json={
        'session_id': session_id,
        'target_column': 'sales',
        'perform_tuning': True
    }
)
print("ML Results:", ml_response.json())

# Make prediction
prediction = requests.post(
    'http://localhost:8000/ml/predict',
    json={
        'session_id': session_id,
        'model_name': 'RandomForestClassifier',
        'new_data': {
            'feature1': 100,
            'feature2': 50,
            'feature3': 'category_a'
        }
    }
)
print("Prediction:", prediction.json())
```

### 4. RAG Document Analysis

```python
# Upload PDF document
with open('report.pdf', 'rb') as f:
    pdf_response = requests.post(
        'http://localhost:8000/agent/execute',
        json={
            'session_id': session_id,
            'prompt': 'Upload and analyze this PDF document',
            'file_path': 'path/to/report.pdf'
        }
    )

# Ask questions about the document
question_response = requests.post(
    'http://localhost:8000/agent/execute',
    json={
        'session_id': session_id,
        'prompt': 'What are the key findings in this report?'
    }
)
print("Answer:", question_response.json())
```

---

## ğŸ› ï¸ Development

### Development Setup
```bash
# Install development dependencies
pip install -r requirements.txt

# Run in development mode
uvicorn backend.api.main:app --reload --host 0.0.0.0 --port 8000
```

### Code Quality
- **Type Hints**: Comprehensive type annotations
- **Error Handling**: Robust error handling strategies
- **Documentation**: Inline documentation dan API docs
- **Testing**: Unit dan integration testing

### Architecture Patterns
- **Modular Design**: Service-based architecture
- **Separation of Concerns**: Clear separation antara layers
- **Dependency Injection**: Loose coupling antara components
- **Error Handling**: Comprehensive error handling

---

## ğŸ“Š Performance

### Benchmarks
- **API Response Time**: < 200ms untuk simple operations
- **ML Training**: < 30s untuk medium datasets
- **RAG Processing**: < 5s untuk document analysis
- **Memory Usage**: Optimized untuk large datasets

### Optimization
- **Caching**: Session-based caching
- **Parallel Processing**: Multi-threaded operations
- **Memory Management**: Efficient memory usage
- **Database Optimization**: Optimized queries

---

## ğŸ”’ Security

### Security Features
- **Input Validation**: Comprehensive input validation
- **File Upload Security**: Secure file handling
- **Session Management**: Secure session handling
- **API Security**: Rate limiting dan authentication

### Best Practices
- **Environment Variables**: Secure configuration
- **File Validation**: File type dan size validation
- **Error Handling**: Secure error messages
- **Data Privacy**: User data protection

---

## ğŸš€ Deployment

### Production Deployment
```bash
# Install production dependencies
pip install -r requirements.txt

# Run production server
uvicorn backend.api.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### Docker Deployment
```dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "backend.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## ğŸ¤ Contributing

### How to Contribute
1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

### Development Guidelines
- Follow PEP 8 style guide
- Add comprehensive tests
- Update documentation
- Ensure backward compatibility

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **FastAPI** untuk web framework
- **LangChain** untuk AI agent framework
- **Scikit-learn** untuk machine learning
- **Pandas** untuk data manipulation
- **Matplotlib/Seaborn** untuk visualization
- **Google Generative AI** untuk LLM capabilities

---

## ğŸ“ Support

### Getting Help
- **Documentation**: Check API documentation at `/docs`
- **Issues**: Report issues on GitHub
- **Discussions**: Join community discussions

### Contact
- **Email**: support@datawhisperer.com
- **GitHub**: [Data Whisperer Repository](https://github.com/your-username/data-whisperer)
- **Documentation**: [Full Documentation](https://datawhisperer.com/docs)

---

## ğŸ”® Roadmap

### Version 1.1 (Planned)
- [ ] React Frontend Interface
- [ ] Advanced Visualization Options
- [ ] Enhanced Memory Management
- [ ] Performance Optimizations

### Version 1.2 (Future)
- [ ] Multi-user Support
- [ ] Advanced ML Models
- [ ] Real-time Collaboration
- [ ] Cloud Deployment

---

**Data Whisperer v1.0** - *Empowering Data Analysis with AI*

*Built with â¤ï¸ for the data science community*
